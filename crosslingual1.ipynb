{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Lambda\n",
    "\n",
    "# import by me\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Embedding, LSTM #, Merge\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "class ModelParam(object):\n",
    "    \"\"\"\n",
    "    define the parameters of the model\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size1, # sentence length of language1\n",
    "        input_size2, # sentence length of language2\n",
    "        vocab_size, \n",
    "        sent_size, \n",
    "        embedding_dim,\n",
    "        sent_vector,\n",
    "    ):\n",
    "        self.input_size1 = input_size1\n",
    "        self.input_size2 = input_size2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sent_size = sent_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sent_vector = sent_vector\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "# initialize some variables\n",
    "w2v_model = KeyedVectors.load_word2vec_format('vectors_embeddings.bin', binary=True)\n",
    "\n",
    "# Inpute size\n",
    "w2v_dim = 200\n",
    "\n",
    "n_units_1st_layer = 64\n",
    "\n",
    "# Training epoch number\n",
    "n_epoch = 100\n",
    "\n",
    "# Model Optimization parameters\n",
    "batch_size = 64\n",
    "gradient_clipping_norm = 1.25\n",
    "\n",
    "# File name (or the intact file path) which indicates the model you want to save.\n",
    "saved_model = \"embeddings_saved_model.hdf5\"\n",
    "\n",
    "# Whether use early stopping\n",
    "# If you turn off early stopping the auc values after each epoch will not be computed.\n",
    "early_stopping_or_not = True\n",
    "\n",
    "# Control parameters of early stopping\n",
    "min_delta_value = 1e-3\n",
    "patience_steps_num =50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "TRAIN_CSV = 'train1.csv'\n",
    "TEST_CSV = 'test.csv'\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "def text_to_word_list(text):\n",
    "    \n",
    "    text = str(text)\n",
    "    text = text.upper()\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "# This is the model for generating predictions\n",
    "# Takes parameters from ModelParam class\n",
    "class crossLingualModel(nn.Module):\n",
    "    def __init__(self, model_param: ModelParam):\n",
    "        super().__init__()\n",
    "        #Ifty\n",
    "        self.embedding = embeddings\n",
    "        self.sent_vector = model_param.sent_vector\n",
    "        \n",
    "        self.sent = torch.randn(\n",
    "            model_param.sent_size, \n",
    "            requires_grad=True, \n",
    "            dtype=torch.float\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(\n",
    "            model_param.embedding_dim + model_param.sent_size,\n",
    "            model_param.vocab_size\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    # x1: embeddings of context words in lang1\n",
    "    # x2: embeddings of context words in lang2\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = torch.FloatTensor(x1)\n",
    "        x2= torch.FloatTensor(x2)\n",
    "        embedding_output1 = x1 \n",
    "        embedding_output1 = F.relu(embedding_output1)\n",
    "        embedding_output2 = x2 \n",
    "        embedding_output2 = F.relu(embedding_output2)\n",
    "        sum_embedding1 = torch.Tensor([x.sum() for x in embedding_output1.transpose(0, -1)])\n",
    "        sum_embedding2 = torch.Tensor([x.sum() for x in embedding_output2.transpose(0, -1)])\n",
    "        sent = self.sent\n",
    "        \n",
    "        concat1 = torch.cat((sum_embedding1, torch.Tensor(self.sent_vector)), 0)\n",
    "        concat2 = torch.cat((sum_embedding2, torch.Tensor(self.sent_vector)), 0)\n",
    "\n",
    "        linear_output1 = self.linear(concat1)\n",
    "        linear_output2 = self.linear(concat2)\n",
    "\n",
    "        pred1 = self.softmax(linear_output1.reshape(1,-1))\n",
    "        pred2 = self.softmax(linear_output2.reshape(1,-1))\n",
    "\n",
    "        return [pred1, pred2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "# For calculating word vector\n",
    "def get_mean_vector(words):\n",
    "    # remove out-of-vocabulary words #w2v_model.wv\n",
    "    words = [word for word in words if word in w2v_model.wv]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(w2v_model.wv[words], axis=0)\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "# For getting center word and surrounding words according to sliding window with window size C\n",
    "def get_windows(words_lang1, words_lang2, C):\n",
    "    i = C\n",
    "    #get word_list with min length\n",
    "    lang1_len = len(words_lang1)\n",
    "    lang2_len = len(words_lang2)\n",
    "    min_len = min(lang1_len, lang2_len)\n",
    "    \n",
    "    while i < min_len - C:\n",
    "        center_word_lang1 = words_lang1[i]\n",
    "        center_word_lang2 = words_lang2[i]\n",
    "        context_words_lang1 = words_lang1[(i - C):i] + words_lang1[(i+1):(i+C+1)]\n",
    "        context_words_lang2 = words_lang2[(i - C):i] + words_lang2[(i+1):(i+C+1)]\n",
    "        yield context_words_lang1, context_words_lang2, center_word_lang1, center_word_lang2 \n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/home/user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "/home/user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/ipykernel_launcher.py:48: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_losses\n",
      "[0.21432772278785706, -0.017687208950519562, 0.18492980301380157, -0.1894727200269699, 0.1078895851969719, -0.09708941727876663, 0.011865071952342987, 0.41190579533576965, -0.1858416646718979, -0.3982364237308502, 0.24906781315803528, 0.8052610158920288, 0.33057349920272827, 0.1024564802646637, 0.1958761066198349, 0.41239234805107117, -0.12415763735771179, 0.07016605138778687, -0.013905540108680725, -0.140476793050766, 0.05203809589147568, -0.10878349095582962, -0.24183279275894165, 0.4411723017692566, -0.5538578033447266, -0.19453489780426025, 0.03746756166219711, -0.28095120191574097]\n",
      "epoch_losses\n",
      "[0.21443015336990356, 0.08213548362255096, 0.16635334491729736, -0.12901253998279572, 0.15858696401119232, -0.07123620063066483, 0.0021004769951105118, 0.4442633390426636, -0.12054423987865448, -0.42788925766944885, 0.2476448267698288, 0.811376690864563, 0.29557451605796814, 0.14119456708431244, 0.20867133140563965, 0.41332074999809265, -0.15096382796764374, 0.14994597434997559, -0.0013118088245391846, -0.16232994198799133, 0.025186659768223763, -0.12666375935077667, -0.28295427560806274, 0.37699708342552185, -0.5539571046829224, -0.15152844786643982, 0.08394046872854233, -0.2583465278148651]\n",
      "epoch_losses\n",
      "[0.2605770230293274, 0.03319057077169418, 0.17240040004253387, -0.167170450091362, 0.14603637158870697, -0.04236534237861633, -0.06287457048892975, 0.39032459259033203, -0.18736591935157776, -0.39518988132476807, 0.2407549023628235, 0.7935103178024292, 0.4164750874042511, 0.07334629446268082, 0.25755563378334045, 0.4142312705516815, -0.16218306124210358, 0.12601971626281738, 0.05140642821788788, -0.17843271791934967, 0.008333899080753326, -0.12044207751750946, -0.24688714742660522, 0.42483487725257874, -0.5419450998306274, -0.13563764095306396, 0.07345689833164215, -0.2509499490261078]\n"
     ]
    }
   ],
   "source": [
    "# Run this cell\n",
    "# This is the main cell for preparing center words and cotext words and iterates over entire dataset\n",
    "epochs = 3\n",
    "window_size = 2\n",
    "TRAIN_CSV = 'train1.csv'\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# 80% of total data in dataset is used for training and rest 20% will be used for testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X = train_test_split (train_df, test_size=0.2 )\n",
    "\n",
    "questions_cols = ['lang1', 'lang2']\n",
    "dataset = train_X\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses = list()\n",
    "    for index, row in dataset.iterrows():\n",
    "        #calculate paragraph vector for entire row\n",
    "        sent_vector = get_mean_vector(text_to_word_list(row))\n",
    "        #print(sent_vector)\n",
    "\n",
    "        #keeping vocab size same as embedding dim\n",
    "        model_param = ModelParam(101, 101, 200, 200, 200, sent_vector)\n",
    "        model = crossLingualModel(model_param)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "     \n",
    "        for context_words_lang1, context_words_lang2, center_word_lang1, center_word_lang2 in get_windows(text_to_word_list(row['lang1']), text_to_word_list(row['lang2']), window_size):\n",
    "            context_words_lang1_array = []\n",
    "            context_words_lang2_array = []\n",
    "            for word_lang1 in context_words_lang1:\n",
    "                lang1_array = np.array(w2v_model.wv[word_lang1])\n",
    "                context_words_lang1_array.append(lang1_array)\n",
    "           \n",
    "            for word_lang2 in context_words_lang2:\n",
    "                lang2_array = np.array(w2v_model.wv[word_lang2])\n",
    "                context_words_lang2_array.append(lang2_array)\n",
    "\n",
    "            prediction1, prediction2 = model(np.array(context_words_lang1_array), np.array(context_words_lang2_array))\n",
    "           \n",
    "            #embedding for target word in lang1\n",
    "            target1 = w2v_model.wv[center_word_lang1]\n",
    "            target1 = torch.from_numpy(target1)\n",
    "            target1 = torch.autograd.Variable(target1)\n",
    "            target1 = target1.reshape(1,-1)\n",
    "            \n",
    "            loss_lang1 = nn.BCEWithLogitsLoss()(prediction1, target1)\n",
    "           \n",
    "            #embedding for target word in lang2\n",
    "            target2 = w2v_model.wv[center_word_lang2]\n",
    "            target2 = torch.from_numpy(target2)\n",
    "            target2 = torch.autograd.Variable(target2)\n",
    "            target2 = target2.reshape(1,-1)\n",
    "            \n",
    "            loss_lang2 = nn.BCEWithLogitsLoss()(prediction2, target2)\n",
    "\n",
    "            summed_loss = loss_lang1 + loss_lang2 \n",
    "            epoch_losses.append(summed_loss.item())\n",
    "            model.zero_grad()\n",
    "            summed_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    print('epoch_losses')\n",
    "    print(epoch_losses)\n",
    "        \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ENDS HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
